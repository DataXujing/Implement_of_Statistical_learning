{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 10\n",
    "batch_size = 200\n",
    "num_classes = 2\n",
    "state_size = 16\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(size=1000000):\n",
    "    X = np.array(np.random.choice(2, size=(size,)))\n",
    "    Y = []\n",
    "    '''根据规则生成Y'''\n",
    "    for i in range(size):   \n",
    "        threshold = 0.5\n",
    "        if X[i-3] == 1:\n",
    "            threshold += 0.5\n",
    "        if X[i-8] == 1:\n",
    "            threshold -=0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return X, np.array(Y)\n",
    "\n",
    "\n",
    "'''生成batch数据'''\n",
    "def gen_batch(raw_data, batch_size, num_step):\n",
    "    raw_x, raw_y = raw_data\n",
    "    data_length = len(raw_x)\n",
    "    batch_patition_length = data_length // batch_size                         # ->5000\n",
    "    data_x = np.zeros([batch_size, batch_patition_length], dtype=np.int32)    # ->(200, 5000)\n",
    "    data_y = np.zeros([batch_size, batch_patition_length], dtype=np.int32)    # ->(200, 5000)\n",
    "    '''填到矩阵的对应位置'''\n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = raw_x[batch_patition_length*i:batch_patition_length*(i+1)]# 每一行取batch_patition_length个数，即5000\n",
    "        data_y[i] = raw_y[batch_patition_length*i:batch_patition_length*(i+1)]\n",
    "    epoch_size = batch_patition_length // num_steps                           # ->5000/5=1000 就是每一轮的大小\n",
    "    for i in range(epoch_size):   # 抽取 epoch_size 个数据\n",
    "        x = data_x[:, i * num_steps:(i + 1) * num_steps]                      # ->(200, 5)\n",
    "        y = data_y[:, i * num_steps:(i + 1) * num_steps]\n",
    "        yield (x, y)    # yield 是生成器，生成器函数在生成值后会自动挂起并暂停他们的执行和状态（最后就是for循环结束后的结果，共有1000个(x, y)）\n",
    "def gen_epochs(n, num_steps):\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(), batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> numclass 2 16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''定义placeholder'''\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name=\"x\")\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='y')\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "init_c = tf.zeros([batch_size, state_size])\n",
    "'''RNN输入'''\n",
    "x_one_hot = tf.one_hot(x, num_classes)\n",
    "rnn_inputs = tf.unstack(x_one_hot, axis=1)\n",
    "\n",
    "print('----> numclass', num_classes, state_size)\n",
    "'''定义RNN cell'''\n",
    "# Input gate: input, previous output, and bias.\n",
    "ix = tf.Variable(tf.truncated_normal([num_classes, state_size], -0.1, 0.1))\n",
    "im = tf.Variable(tf.truncated_normal([state_size, state_size], -0.1, 0.1))\n",
    "ib = tf.Variable(tf.zeros([1, state_size]))\n",
    "# Forget gate: input, previous output, and bias.\n",
    "fx = tf.Variable(tf.truncated_normal([num_classes, state_size], -0.1, 0.1))\n",
    "fm = tf.Variable(tf.truncated_normal([state_size, state_size], -0.1, 0.1))\n",
    "fb = tf.Variable(tf.zeros([1, state_size]))\n",
    "# Memory cell: input, state and bias.                             \n",
    "cx = tf.Variable(tf.truncated_normal([num_classes, state_size], -0.1, 0.1))\n",
    "cm = tf.Variable(tf.truncated_normal([state_size, state_size], -0.1, 0.1))\n",
    "cb = tf.Variable(tf.zeros([1, state_size]))\n",
    "# Output gate: input, previous output, and bias.\n",
    "ox = tf.Variable(tf.truncated_normal([num_classes, state_size], -0.1, 0.1))\n",
    "om = tf.Variable(tf.truncated_normal([state_size, state_size], -0.1, 0.1))\n",
    "ob = tf.Variable(tf.zeros([1, state_size]))\n",
    "# Variables saving state across unrollings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell(rnn_input, h, c):\n",
    "    # a = tf.matmul(rnn_input, ix)\n",
    "    # print('--a', a.get_shape())\n",
    "    # a =  tf.matmul(h, im)\n",
    "    # print('--a', a.get_shape())\n",
    "    input_gate = tf.sigmoid(tf.matmul(rnn_input, ix) + tf.matmul(h, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(rnn_input, fx) + tf.matmul(h, fm) + fb)\n",
    "    update = tf.matmul(rnn_input, cx) + tf.matmul(c, cm) + cb\n",
    "    state = forget_gate * c + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(rnn_input, ox) + tf.matmul(h, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----> fianl state (200, 16)\n"
     ]
    }
   ],
   "source": [
    "state = init_state\n",
    "c = init_c\n",
    "rnn_outputs = []\n",
    "for rnn_input in rnn_inputs:\n",
    "    state, c = lstm_cell(rnn_input, state, c)  # state会重复使用，循环\n",
    "    rnn_outputs.append(state)\n",
    "final_state = rnn_outputs[-1]  # 得到最后的state\n",
    "print('----> fianl state', final_state.get_shape())\n",
    "\n",
    "# cell = tf.contrib.rnn.BasicRNNCell(num_units=state_size)\n",
    "# rnn_outputs, final_state = tf.contrib.rnn.static_rnn(cell=cell, inputs=rnn_inputs,\n",
    "# initial_state=init_state)\n",
    "# rnn_outputs, final_state = tf.nn.dynamic_rnn(cell=cell, inputs=rnn_inputs,\n",
    "# initial_state=init_state)\n",
    "\n",
    "\n",
    "'''预测，损失，优化'''\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "y_as_list = tf.unstack(y, num=num_steps, axis=1)\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logit) for logit, label in\n",
    "          zip(logits, y_as_list)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
